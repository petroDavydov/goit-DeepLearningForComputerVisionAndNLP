# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import roc_auc_score
import spacy
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
import string
import itertools
from collections import Counter

import pandas as pd
import numpy as np
import spacy  # –¥–æ–¥–∞–Ω–æ –¥–ª—è —Ä–æ–±–æ—Ç–∏ —É –ø–∞–π—Ç–æ–Ω –ª–æ–∫–∞–ª—å–Ω–æ

import nltk
# nltk.download('stopwords')  —É–≤—ñ–º–∫–Ω—É—Ç–∏ —è–∫—â–æ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
# –ø–µ—Ä–µ–≤—ñ—Ä—è—î –ª–æ–∫–∞–ª—å–Ω—É –ø–∞–ø–∫—É, –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –Ω—ñ—á–æ–≥–æ –∑ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É
stopwords.words("english")

tqdm.pandas()


df = pd.read_csv('./Reviews.csv', index_col='Id')

df_head = df.head(5)
print(f"Thi is dataframe head with index_col='Id: \n ' {df_head}")

df_info = df.info()
print(f"Thi is dataframe info with index_col='Id: \n ' {df_info}")

df_loc = df.loc[df['Score'] != 3]
print(f"Thi is dataframe loc with index_col='Id: \n ' {df_loc}")

df_shape = df_loc.shape
print(f"Thi is dataframe_loc shape with index_col='Id: \n ' {df_shape}")


df['sentiment'] = [1 if score in [4, 5] else 0 for score in df['Score']]
print(f"Thi is dataframe sentiment ' {df['sentiment'].head(10)}")


print("\n–í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤\n")

df_sum_duplicates = df.duplicated().sum()
print(f"Thi is dataframe duplicated sum ü§ì \n' {df_sum_duplicates}")

print("\n–í–∏–¥–∞–ª–∏–º–æ –∑–∞–ø–∏—Å–∏-–¥—É–±–ª—ñ–∫–∞—Ç–∏\n")

df_duplicates_remove = df.drop_duplicates().reset_index(drop=True)
print(f"Remove duplicates: \n {df_duplicates_remove}")


print("\n–ü–æ—à—É–∫–∞—î–º–æ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ –≤—ñ–¥–≥—É–∫–∏ –ø—Ä–æ —Ä—ñ–∑–Ω—ñ –≤–µ—Ä—Å—ñ—ó –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç—É.\n")

df_groupby_reviews = df.groupby(['UserId', 'Time', 'Text']).count(
).sort_values('ProductId', ascending=False).head(10)
print(f"Group by reviws: \n {df_groupby_reviews}")

print("\n–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ –¥—É–±–ª—é–≤–∞–Ω–Ω—è –≤—ñ–¥–≥—É–∫—ñ–≤\n")

search_text = "I have two cats, one 6 and one 2 years old. Both are indoor cats in excellent health. I saw the negative review and talked to my vet about it. I've also asked a number of veterinary professionals what to feed my cats and they all answer the same thing: Science Diet. Sure, you'll see stories of how one person's cat had issues, but even if that's 100% true, it's 1 case out of millions. Science and fact aren't based on someone's experience.<br /><br />So my point is, I love my cats and I'm very concerned about their health. I trust people who actually have medical degrees and experience with a wide range of animals. My only caution is do not fall for some hype or scare tactic that recommends some unproven or untested food or some fad diet for your pet. Don't listen to me, don't listen to the negative review. ASK YOUR VET what they recommend, and follow their instructions. My guess is you'll end up buying the Science Diet anyhow."
duplicates_example = df.loc[
    (df['UserId'] == 'A36JDIN9RAAIEC') &
    (df['Time'] == 1292976000) &
    (df['Text'] == search_text)
]
print(f"Example of duplicates: \n {duplicates_example}")


print("\n–í–∏–¥–∞–ª–∏–º–æ –æ–¥–Ω–∞–∫–æ–≤—ñ –≤—ñ–¥–≥—É–∫–∏\n")

df_remove_double_review = df.drop_duplicates(subset={"UserId", "Time", "Text"})

print(f"Print double reviews üåë: \n{df_remove_double_review.shape}")

print(f"\n–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (text normalization)\n")

contractions = {
    "ain't": "am not",
    "aren't": "are not",
    "can't": "cannot",
    "can't've": "cannot have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would",
    "he'd've": "he would have",
    "he'll": "he will",
    "he's": "he is",
    "how'd": "how did",
    "how'll": "how will",
    "how's": "how is",
    "i'd": "i would",
    "i'll": "i will",
    "i'm": "i am",
    "i've": "i have",
    "isn't": "is not",
    "it'd": "it would",
    "it'll": "it will",
    "it's": "it is",
    "let's": "let us",
    "ma'am": "madam",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "must've": "must have",
    "mustn't": "must not",
    "needn't": "need not",
    "oughtn't": "ought not",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "she'd": "she would",
    "she'll": "she will",
    "she's": "she is",
    "should've": "should have",
    "shouldn't": "should not",
    "that'd": "that would",
    "that's": "that is",
    "there'd": "there had",
    "there's": "there is",
    "they'd": "they would",
    "they'll": "they will",
    "they're": "they are",
    "they've": "they have",
    "wasn't": "was not",
    "we'd": "we would",
    "we'll": "we will",
    "we're": "we are",
    "we've": "we have",
    "weren't": "were not",
    "what'll": "what will",
    "what're": "what are",
    "what's": "what is",
    "what've": "what have",
    "where'd": "where did",
    "where's": "where is",
    "who'll": "who will",
    "who's": "who is",
    "won't": "will not",
    "wouldn't": "would not",
    "you'd": "you would",
    "you'll": "you will",
    "you're": "you are"
}


print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –ø–µ—Ä–µ–ª—ñ–∫ —Å—Ç–æ–ø-—Å–ª—ñ–≤ —ñ–∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ nltk.\n")

stop_words = set(stopwords.words('english')).union(
    {'also', 'would', 'much', 'many'})


print(f"\n–í–∏–¥–∞–ª–∏–º–æ —Å–ª–æ–≤–∞-–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –∑—ñ —Å–ø–∏—Å–∫—É —Å—Ç–æ–ø-—Å–ª—ñ–≤.\n")

negations = {
    'aren',
    "aren't",
    'couldn',
    "couldn't",
    'didn',
    "didn't",
    'doesn',
    "doesn't",
    'don',
    "don't",
    'hadn',
    "hadn't",
    'hasn',
    "hasn't",
    'haven',
    "haven't",
    'isn',
    "isn't",
    'mightn',
    "mightn't",
    'mustn',
    "mustn't",
    'needn',
    "needn't",
    'no',
    'nor',
    'not',
    'shan',
    "shan't",
    'shouldn',
    "shouldn't",
    'wasn',
    "wasn't",
    'weren',
    "weren't",
    'won',
    "won't",
    'wouldn',
    "wouldn't"
}

stop_words = stop_words.difference(negations)

print(f"\n–°—Ç–µ–º—ñ–Ω–≥ - –∑–∞–∑–≤–∏—á–∞–π —î —à–≤–∏–¥—à–∏–º —ñ –º–µ–Ω—à —Å–∫–ª–∞–¥–Ω–∏–º –∑–∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—î—é, —è–∫–∞ –≤—Ä–∞—Ö–æ–≤—É—î –º–æ—Ä—Ñ–æ–ª–æ–≥—ñ—á–Ω—ñ –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ –º–æ–≤–∏.\n")

stemmer = PorterStemmer()


print("######################################")

print(f"\n–†–µ–≥—É–ª—è—Ä–Ω—ñ –≤–∏—Ä–∞–∑–∏ (regular expressions, RegEx).\n")


print(f"\n–ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –ø–∞–π–ø–ª–∞–π–Ω –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ spacy.\n")
# —è–∫—â–æ –Ω–µ –ø—Ä–∞—Ü—é—î –≤–∏–∫–æ–Ω–∞—Ç–∏: python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])


print(f"\n–í–∏–∑–Ω–∞—á–∏–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É.\n")


def normalize_text(raw_review):

    # Remove html tags
    # match <> and everything in between. [^>] - match everything except >
    text = re.sub("<[^>]*>", " ", raw_review)

    # Remove emails
    # match non-whitespace characters, @ and a whitespaces in the end
    text = re.sub("\\S*@\\S*[\\s]+", " ", text)

    # remove links
    # match http, s - zero or once, //,
    text = re.sub("https?:\\/\\/.*?[\\s]+", " ", text)
    # any char 0-unlimited, whitespaces in the end

    # Convert to lower case, split into individual words
    text = text.lower().split()

    # Replace contractions with their full versions
    text = [contractions.get(word, word) if word in contractions else word  # (word) --> (word,word)
            for word in text]

    # Re-splitting for the correct stop-words extraction
    text = " ".join(text).split()

    # Remove stop words
    text = [word for word in text if not word in stop_words]

    text = " ".join(text)

    # Remove non-letters
    # match everything except letters and '
    text = re.sub("[^a-zA-Z' ]", "", text)

    # Stem words. Need to define porter stemmer above
    # text = [stemmer.stem(word) for word in text.split()]

    # Lemmatize words. Need to define lemmatizer above
    doc = nlp(text)
    text = " ".join([token.lemma_ for token in doc if len(token.lemma_) > 1])

    # Remove excesive whitespaces
    text = re.sub("[\\s]+", " ", text)

    # Join the words back into one string separated by space, and return the result.
    return text


print(f"\n–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —ó—ó –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ.\n")

text = """On a quest for the perfedc1112t,,, !!!! <br />%%2%% popcorn to \ncompliment the Whirley Pop.  Don\'t get older, I\'m beginning to \nappreciate the more "natural" popcorn varieties, and I suppose\n that\'s what attracted me to the Arrowhead Mills Organic Yellow\n Popcorn.<br /> <br />I\'m no "organic" food expert.  I just wanted\n some good tasting popcorn.  And, I feel like that\'s what I got.\n  Using the Whirley Pop, with a very small amount of oil, I\'ve had \ngreat results.\n"""  # –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Ç–µ–∫—Å—Ç, –∑–∞–º—ñ–Ω–µ–Ω–æ –ª–∞–ø–∫–∏ –Ω–∞ —Ç—Ä–æ–π–Ω—ñ


print('Original text: ', text, '#'*30, sep='\n')

# -----------------–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ, –¥–ª—è —Ä–æ–±–æ—Ç–∏ —É –ø–∞–π—Ç–æ–Ω----
normalized = normalize_text(text)

words = normalized.split()
chunks = [" ".join(words[i:i+9]) for i in range(0, len(words), 10)]

formatted = "\n\n".join(chunks)

print("\nNormalized text:\n", formatted, sep="")


print(f"–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∏—Ö —Ü—ñ–ª–µ–π –∑–º–µ–Ω—à–∏–º–æ —Ä–æ–∑–º—ñ—Ä –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –¥–æ 5 —Ç–∏—Å—è—á –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")

df = df.groupby('sentiment').sample(2500, random_state=42)

print(f"\n–î–∏–≤–∏–º–æ—Å—å –Ω–∞ –∑–º–µ–Ω—à–µ–Ω–∏–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–æ 5000: \n {df.shape}")


print(f"\n–ó–∞—Å—Ç–æ—Å—É—î–º–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ –¥–æ —Ç–µ–∫—Å—Ç—É –æ–≥–ª—è–¥—ñ–≤.\n")

df['text_normalized'] = df['Text'].progress_apply(normalize_text)

print("##############################")
print(f"\n–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ üêù\n")

print(f"\nBag of Words\n")

print(f"\n–†–æ–∑–¥—ñ–ª–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –π —Ç–µ—Å—Ç–æ–≤—ñ\n")

train_idxs = df.sample(frac=0.8, random_state=42).index
test_idxs = [idx for idx in df.index if idx not in train_idxs]
X_train = df.loc[train_idxs, 'text_normalized']
X_test = df.loc[test_idxs, 'text_normalized']

y_train = df.loc[train_idxs, 'sentiment']
y_test = df.loc[test_idxs, 'sentiment']

print(f"\n–°—Ç–≤–æ—Ä—é—î–º–æ –π –Ω–∞–≤—á–∞—î–º–æ –æ–±'—î–∫—Ç CountVectorizer\n")

vect = CountVectorizer().fit(X_train)

# len(vect.vocabulary_)
print(f"\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤:\n {len(vect.vocabulary_)}\n")

print(f"\n–ü–æ–¥–∏–≤–∏–º–æ—Å—å –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥ –æ–∑–Ω–∞–∫, —è–∫—ñ –±—É–ª–æ –≤–∏–æ–∫—Ä–µ–º–ª–µ–Ω–æ. \n")

print(vect.get_feature_names_out()[:5])

print(f"\n–ü–µ—Ä–µ—Ç–≤–æ—Ä–∏–º–æ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—é –¥–æ–∫—É–º–µ–Ω—Ç-—Ç–µ—Ä–º—ñ–Ω (document-term matrix).\n")

# –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞
print(f"\n–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞\n")

print(type(vect))

print(type(X_test))


X_train_vectorized = vect.transform(X_train)
print(X_train_vectorized.shape)
print(f"\nThis is X_traine_vectorized: \n{X_train_vectorized}")

print(f"\n–°—Ç–≤–æ—Ä–∏–º–æ –∫–ª–∞—Å –º–æ–¥–µ–ª—ñ –π –Ω–∞–≤—á–∏–º–æ —ó—ó –Ω–∞ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö.\n")

model = LogisticRegression(random_state=42)
model.fit(X_train_vectorized, y_train)


print(f"\n–†–æ–∑—Ä–∞—Ö—É—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ.\n")

predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))

print(f"\n–î–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö –Ω–∞–ø–∏—à–µ–º–æ –¥–æ–ø–æ–º—ñ–∂–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é.\n")


def get_preds(text_column, algorithm, ngrams=(1, 1)):

    X_train = df.loc[train_idxs, text_column]
    X_test = df.loc[test_idxs, text_column]

    y_train = df.loc[train_idxs, 'sentiment']
    y_test = df.loc[test_idxs, 'sentiment']

    if algorithm == 'cv':
        vect = CountVectorizer(ngram_range=ngrams).fit(X_train)
    elif algorithm == 'tfidf':
        vect = TfidfVectorizer(ngram_range=ngrams).fit(X_train)
    else:
        raise ValueError('Select correct algorithm: `cv` or `tfidf`')

    print('Vocabulary length: ', len(vect.vocabulary_))

    # transform the documents in the training data to a document-term matrix

    X_train_vectorized = vect.transform(X_train)
    print('Document-term matrix shape:', X_train_vectorized.shape)

    model = LogisticRegression(random_state=42)
    model.fit(X_train_vectorized, y_train)

    predictions = model.predict(vect.transform(X_test))

    print('AUC: ', roc_auc_score(y_test, predictions))


print(f"\n–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –Ω–∞ –Ω–µ—Ç–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö.\n")

print(f"\n –ù–µ—Ç–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ: \n {get_preds('Text', 'cv')}\n")


print("##########################")
print(f"\nTF-IDF: Term frequency-inverse document frequency (tf-idf)\n")

print(f"\n–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –º–µ—Ç–æ–¥ TF-IDF.\n")

print(f"\nThis is normal data: \n{get_preds('text_normalized', 'tfidf')}")

print("\n–î–ª—è –Ω–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö:\n")

print(f"\nThis is not-normal data: \n{get_preds('Text', 'tfidf')}")

print("###########################")

print(f"\nN-Grams\n")

get_preds('text_normalized', 'cv', (1, 2))

get_preds('text_normalized', 'tfidf', (1, 2))

get_preds('text_normalized', 'cv', (2, 2))

get_preds('Text', 'cv', (2, 2))

get_preds('Text', 'tfidf', (2, 2))
