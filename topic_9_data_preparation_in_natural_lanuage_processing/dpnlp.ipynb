{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/petroDavydov/goit-DeepLearningForComputerVisionAndNLP/blob/main/dpnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLw9Q3nKXg1B"
   },
   "source": [
    "# ***Topic 9. Data preparation in natural language processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaP3OAAkYUkJ"
   },
   "source": [
    "–ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv-5E_QWQt_k"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cqmkOQEYyz6"
   },
   "source": [
    "–ó—á–∏—Ç–∞—î–º–æ –Ω–∞—à –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK_4sD5qY0oX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/DeepLearningforComputervisionandNLP/Reviews.csv', index_col='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePcnGXMxeDQF"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k2G1jMpgR0p"
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWBrHZXfgUXp"
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['Score']!=3]\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqv7sedDgW4T"
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = [1 if score in [4, 5] else 0 for score in df['Score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x0MHjH0gdpp"
   },
   "source": [
    "–í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoqrsYjegbBf"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cMG7YBQgm-1"
   },
   "source": [
    "–í–∏–¥–∞–ª–∏–º–æ –∑–∞–ø–∏—Å–∏-–¥—É–±–ª—ñ–∫–∞—Ç–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rft3CRlygndi"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVVmaDHLgsq7"
   },
   "source": [
    "–ü–æ—à—É–∫–∞—î–º–æ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ –≤—ñ–¥–≥—É–∫–∏ –ø—Ä–æ —Ä—ñ–∑–Ω—ñ –≤–µ—Ä—Å—ñ—ó –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdJHgb_TgoJm"
   },
   "outputs": [],
   "source": [
    "df.groupby(['UserId', 'Time', 'Text']).count().sort_values('ProductId', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "728DFVVFg5mC"
   },
   "outputs": [],
   "source": [
    "search_text = \"I have two cats, one 6 and one 2 years old. Both are indoor cats in excellent health. I saw the negative review and talked to my vet about it. I've also asked a number of veterinary professionals what to feed my cats and they all answer the same thing: Science Diet. Sure, you'll see stories of how one person's cat had issues, but even if that's 100% true, it's 1 case out of millions. Science and fact aren't based on someone's experience.<br /><br />So my point is, I love my cats and I'm very concerned about their health. I trust people who actually have medical degrees and experience with a wide range of animals. My only caution is do not fall for some hype or scare tactic that recommends some unproven or untested food or some fad diet for your pet. Don't listen to me, don't listen to the negative review. ASK YOUR VET what they recommend, and follow their instructions. My guess is you'll end up buying the Science Diet anyhow.\"\n",
    "duplicates_example = df.loc[\n",
    "    (df['UserId']=='A36JDIN9RAAIEC') &\n",
    "    (df['Time']==1292976000) &\n",
    "    (df['Text']==search_text)\n",
    "]\n",
    "\n",
    "duplicates_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-xrQtDkhBc4"
   },
   "source": [
    "–í–∏–¥–∞–ª–∏–º–æ –æ–¥–Ω–∞–∫–æ–≤—ñ –≤—ñ–¥–≥—É–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3Kc3UBWhCbp"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset={\"UserId\", \"Time\",\"Text\"})\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzrJDPUuhEj9"
   },
   "source": [
    "# ***–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (text normalization)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeVvwjH2N5GM"
   },
   "source": [
    "–ü—ñ–¥–≥–æ—Ç—É—î–º–æ –¥–æ–ø–æ–º—ñ–∂–Ω—ñ —Å–ø–∏—Å–∫–∏ –¥–∞–Ω–∏—Ö, —è–∫—ñ –∑–Ω–∞–¥–æ–±–ª—è—Ç—å—Å—è –Ω–∞–º –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyv_mRtrLJZA"
   },
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzzUd7KcOApb"
   },
   "source": [
    "–ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –ø–µ—Ä–µ–ª—ñ–∫ —Å—Ç–æ–ø-—Å–ª—ñ–≤ —ñ–∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ nltk. –ë—É–¥–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–∏—Ö set() –¥–ª—è –±—ñ–ª—å—à —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ –¥–∞–Ω–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvQutPGkOBLu"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({'also', 'would', 'much', 'many'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR3h-F9lODkr"
   },
   "source": [
    "–í–∏–¥–∞–ª–∏–º–æ —Å–ª–æ–≤–∞-–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –∑—ñ —Å–ø–∏—Å–∫—É —Å—Ç–æ–ø-—Å–ª—ñ–≤. –ó–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è –º–æ–∂—É—Ç—å –º–∞—Ç–∏ –∫–ª—é—á–æ–≤—É —Ä–æ–ª—å —É –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—ñ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç—É. –¢–∞–∫, —è–∫—â–æ –≤ —Ä–µ—á–µ–Ω–Ω—ñ ¬´I don‚Äôt like this product¬ª –º–∏ –≤–∏–¥–∞–ª–∏–º–æ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è ¬´don‚Äôt¬ª, —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Ä–µ—á–µ–Ω–Ω—è –æ–¥—Ä–∞–∑—É –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç—å—Å—è –∑ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQDzVYCoOKCM"
   },
   "outputs": [],
   "source": [
    "negations = {\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'shan',\n",
    "    \"shan't\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\"\n",
    "}\n",
    "stop_words = stop_words.difference(negations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efksBvECOXaS"
   },
   "source": [
    "–î–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Å—Ç–µ–º—ñ–Ω–≥—É –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –∫–ª–∞—Å PorterStemmer –∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OehOVSiPOX8J"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLYoj8wqOZrl"
   },
   "source": [
    "# ***–†–µ–≥—É–ª—è—Ä–Ω—ñ –≤–∏—Ä–∞–∑–∏ (regular expressions, RegEx)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISIhiXrW60Q0"
   },
   "source": [
    "–ó–∞–≤–∞–Ω—Ç–∞–∂–∏–º–æ –ø–∞–π–ø–ª–∞–π–Ω –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llvmmgl_Q_YF"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser','ner'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AtMommz66q6"
   },
   "source": [
    "–í–∏–∑–Ω–∞—á–∏–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJiKJhhu67vw"
   },
   "outputs": [],
   "source": [
    "def normalize_text(raw_review):\n",
    "\n",
    "    # Remove html tags\n",
    "    text = re.sub(\"<[^>]*>\", \" \", raw_review) # match <> and everything in between. [^>] - match everything except >\n",
    "\n",
    "    # Remove emails\n",
    "    text = re.sub(\"\\\\S*@\\\\S*[\\\\s]+\", \" \", text) # match non-whitespace characters, @ and a whitespaces in the end\n",
    "\n",
    "    # remove links\n",
    "    text = re.sub(\"https?:\\\\/\\\\/.*?[\\\\s]+\", \" \", text) # match http, s - zero or once, //,\n",
    "                                                    # any char 0-unlimited, whitespaces in the end\n",
    "\n",
    "     # Convert to lower case, split into individual words\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Replace contractions with their full versions\n",
    "    text = [contractions.get(word) if word in contractions else word\n",
    "            for word in text]\n",
    "\n",
    "    # Re-splitting for the correct stop-words extraction\n",
    "    text = \" \".join(text).split()\n",
    "\n",
    "    # Remove stop words\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text) # match everything except letters and '\n",
    "\n",
    "    # Stem words. Need to define porter stemmer above\n",
    "    # text = [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "    # Lemmatize words. Need to define lemmatizer above\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc if len(token.lemma_) > 1 ])\n",
    "\n",
    "    # Remove excesive whitespaces\n",
    "    text = re.sub(\"[\\\\s]+\", \" \", text)\n",
    "\n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-NK7cZQ7eh_"
   },
   "source": [
    "–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —ó—ó –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ3_s9Mx7fL7"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"On a quest for the perfedc1112t,,, !!!! <br />%%2%% popcorn to \\ncompliment the Whirley Pop.  Don\\'t get older, I\\'m beginning to \\nappreciate the more \"natural\" popcorn varieties, and I suppose\\n that\\'s what attracted me to the Arrowhead Mills Organic Yellow\\n Popcorn.<br /> <br />I\\'m no \"organic\" food expert.  I just wanted\\n some good tasting popcorn.  And, I feel like that\\'s what I got.\\n  Using the Whirley Pop, with a very small amount of oil, I\\'ve had \\ngreat results.\\n\"\"\"\n",
    "\n",
    "\n",
    "print('Original text: ', text, '#'*30, sep='\\n')\n",
    "\n",
    "# -----------------\n",
    "# normalized = normalize_text(text)\n",
    "\n",
    "# words = normalized.split()\n",
    "# chunks = [\" \".join(words[i:i+9]) for i in range(0, len(words), 10)]\n",
    "\n",
    "# formatted = \"\\n\\n\".join(chunks)\n",
    "\n",
    "# print(\"\\nNormalized text:\\n\", formatted, sep=\"\")\n",
    "\n",
    "\n",
    "print('\\nNormalized text: ', normalize_text(text ), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djCdyFJd-hOl"
   },
   "source": [
    "–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∏—Ö —Ü—ñ–ª–µ–π –∑–º–µ–Ω—à–∏–º–æ —Ä–æ–∑–º—ñ—Ä –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –¥–æ 5 —Ç–∏—Å—è—á –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBLrHfSr-h9w"
   },
   "outputs": [],
   "source": [
    "df = df.groupby('sentiment').sample(2500, random_state=42)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlVMFXz1-pTf"
   },
   "source": [
    "–ó–∞—Å—Ç–æ—Å—É—î–º–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ –¥–æ —Ç–µ–∫—Å—Ç—É –æ–≥–ª—è–¥—ñ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLiBn9_x-qdO"
   },
   "outputs": [],
   "source": [
    "df['text_normalized'] = df['Text'].progress_apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XArOVoJJLTZo"
   },
   "source": [
    "# ***–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UmOliBKjYhX"
   },
   "source": [
    "# ***Bag of Words***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzccSC-qj9j6"
   },
   "source": [
    "–†–æ–∑–¥—ñ–ª–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –π —Ç–µ—Å—Ç–æ–≤—ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TURR9EhiLVGo"
   },
   "outputs": [],
   "source": [
    "train_idxs = df.sample(frac=0.8, random_state=42).index\n",
    "test_idxs = [idx for idx in df.index if idx not in train_idxs]\n",
    "X_train = df.loc[train_idxs, 'text_normalized']\n",
    "X_test = df.loc[test_idxs, 'text_normalized']\n",
    "\n",
    "y_train = df.loc[train_idxs, 'sentiment']\n",
    "y_test = df.loc[test_idxs, 'sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LazlsLbFlg3m"
   },
   "source": [
    "–°—Ç–≤–æ—Ä—é—î–º–æ –π –Ω–∞–≤—á–∞—î–º–æ –æ–±'—î–∫—Ç CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbotNmw8lZuk"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "len(vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU6i1_WXlkNs"
   },
   "source": [
    "–ü–æ–¥–∏–≤–∏–º–æ—Å—å –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥ –æ–∑–Ω–∞–∫, —è–∫—ñ –±—É–ª–æ –≤–∏–æ–∫—Ä–µ–º–ª–µ–Ω–æ. –¢—É—Ç –æ–∑–Ω–∞–∫–æ—é –±—É–¥–µ –∫–æ–∂–µ–Ω —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Ç–æ–∫–µ–Ω —Å–ª–æ–≤–Ω–∏–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8155x2_llQe"
   },
   "outputs": [],
   "source": [
    "vect.get_feature_names_out()[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXp-AQvQlqHn"
   },
   "source": [
    "–ü–µ—Ä–µ—Ç–≤–æ—Ä–∏–º–æ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—é –¥–æ–∫—É–º–µ–Ω—Ç-—Ç–µ—Ä–º—ñ–Ω (document-term matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIC0AO38lq1j"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(X_train_vectorized.shape)\n",
    "X_train_vectorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7imBUHQVneyz"
   },
   "source": [
    "–°—Ç–≤–æ—Ä–∏–º–æ –∫–ª–∞—Å –º–æ–¥–µ–ª—ñ –π –Ω–∞–≤—á–∏–º–æ —ó—ó –Ω–∞ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6S-_boRfnfUT"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_vectorized, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdvFvpJeoM3A"
   },
   "source": [
    "–†–æ–∑—Ä–∞—Ö—É—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z90A_5sIoNbq"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6KU79I4oT3a"
   },
   "source": [
    "–î–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö –Ω–∞–ø–∏—à–µ–º–æ –¥–æ–ø–æ–º—ñ–∂–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é.\n",
    "\n",
    "\n",
    "\n",
    "* üëâüèª –ê–ª–≥–æ—Ä–∏—Ç–º tf-idf –º–∏ —Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –≤ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—ñ.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeE_2622oZct"
   },
   "outputs": [],
   "source": [
    "def get_preds(text_column, algorithm, ngrams=(1,1)):\n",
    "\n",
    "    X_train = df.loc[train_idxs, text_column]\n",
    "    X_test = df.loc[test_idxs, text_column]\n",
    "\n",
    "    y_train = df.loc[train_idxs, 'sentiment']\n",
    "    y_test = df.loc[test_idxs, 'sentiment']\n",
    "\n",
    "    if algorithm == 'cv':\n",
    "        vect = CountVectorizer(ngram_range=ngrams).fit(X_train)\n",
    "    elif algorithm == 'tfidf':\n",
    "        vect = TfidfVectorizer(ngram_range=ngrams).fit(X_train)\n",
    "    else:\n",
    "        raise ValueError('Select correct algorithm: `cv` or `tfidf`')\n",
    "\n",
    "    print('Vocabulary length: ', len(vect.vocabulary_))\n",
    "\n",
    "    # transform the documents in the training data to a document-term matrix\n",
    "\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    print('Document-term matrix shape:', X_train_vectorized.shape)\n",
    "\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy_57LiEodxw"
   },
   "source": [
    "–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –Ω–∞ –Ω–µ—Ç–æ–∫–µ–Ω—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x65V0WdgoeTn"
   },
   "outputs": [],
   "source": [
    "get_preds('Text', 'cv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFLw3xBM3Vi1"
   },
   "source": [
    "# ***TF-IDF***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3jP48KJ3l1T"
   },
   "source": [
    "–í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –º–µ—Ç–æ–¥ TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ixdUrNDunW6"
   },
   "outputs": [],
   "source": [
    "get_preds('text_normalized', 'tfidf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hvxv3vE3seO"
   },
   "source": [
    "–î–ª—è –Ω–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q48LQUm3tZT"
   },
   "outputs": [],
   "source": [
    "get_preds('Text', 'tfidf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clq5AZxT9nLm"
   },
   "source": [
    "# ***N-Grams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzQz3N2s9osm"
   },
   "outputs": [],
   "source": [
    "get_preds('text_normalized', 'cv', (1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPkMngnj-8ww"
   },
   "outputs": [],
   "source": [
    "get_preds('text_normalized', 'tfidf', (1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xsmvanE_DtO"
   },
   "outputs": [],
   "source": [
    "get_preds('text_normalized', 'cv', (2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ3cC6mv_HmX"
   },
   "outputs": [],
   "source": [
    "get_preds('Text', 'cv', (2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuAqIQtH_KvH"
   },
   "outputs": [],
   "source": [
    "get_preds('Text', 'tfidf', (2,2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO5mHUw4AbeT7BGEsu37tBe",
   "include_colab_link": true,
   "mount_file_id": "1v4JX-9OF0XkCot10-bA46rWPHPzQ3CYd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
