{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/petroDavydov/goit-DeepLearningForComputerVisionAndNLP/blob/main/convolutional_neural_networks_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0NJDYUaM0Lb"
   },
   "source": [
    "# ***Convolution Neural Networks***\n",
    "### Згорткові нейронні мережі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF-QXbQV3O6Z",
    "outputId": "d2ac3f25-0146-4df6-bf6a-db1a5a753722"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah4lKOfw3O6a",
    "outputId": "c9b5b9d3-c072-47f4-d267-c1e2ce007a25"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF4biwW2Nun9"
   },
   "source": [
    "Приклад матриці, зображення 3х3 у відтінках сірого:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 128 & 255 \\\\\n",
    "64 & 192 & 32 \\\\\n",
    "255 & 128 & 0\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUKj4NRh3O6a",
    "outputId": "348ba3e9-05ca-4f29-85f1-8b8671a0fbe4"
   },
   "outputs": [],
   "source": [
    "data =[[0,128,255], [64,192,32], [255,128,0]]\n",
    "plt.imshow(data, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz0xRU8ARH7Q"
   },
   "source": [
    "Приклад матриці, зображення 2х2 у колорова матриця:\n",
    "\n",
    "\n",
    "Червоний колір:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "255 & 0 \\\\\n",
    "128 & 64\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "Зелений колір:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "128 & 255 \\\\\n",
    "64 & 0\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "Синій колір:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 128 \\\\\n",
    "255 & 192\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtT_QOqg3O6b",
    "outputId": "0313325c-2c02-4bd0-abb3-a9162f5f1e80"
   },
   "outputs": [],
   "source": [
    "data = [[[255, 0], [128, 64]], [[128, 255], [64, 0]], [[0, 128], [255, 192]]]\n",
    "data = np.moveaxis(data, 0, -1)\n",
    "plt.imshow(data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gV52r0Tb_IJ"
   },
   "source": [
    "# Вирішуємо задачу багатокласової класифікації зображень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBQDyy0x3O6b",
    "outputId": "6b7ff2f3-fa18-4ddf-e583-b7fa5ac0482f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxQve0kX3O6c"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/DeepLearningforComputervisionandNLP/SIGNS_dataset'\n",
    "splits =['train','test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow4HD8sKebFq"
   },
   "source": [
    "# # Display images examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjAnUmtG3O6c",
    "outputId": "640435f0-c1a1-449f-8c1c-a802642fdf9c"
   },
   "outputs": [],
   "source": [
    "# iterate over train and test folders\n",
    "for s in splits:\n",
    "\t\t# list files in the folder with the jpg extension\n",
    "  files = [f for f in os.listdir(f\"{data_path}/{s}_signs\") if f.endswith('.jpg')]\n",
    "  print(f'{len(files)} images in {s}')\n",
    "\n",
    "  # for each image, create a list of the type [class, filename]\n",
    "  files = [f.split('_', 1) for f in files]\n",
    "\n",
    "  # group the data by class\n",
    "  files_by_sign = defaultdict(list)\n",
    "  for k, v in files:\n",
    "    files_by_sign[k].append(v)\n",
    "\n",
    "  # take random 4 images of each class\n",
    "  for k, v in sorted(files_by_sign.items()):\n",
    "    print(f'Number of examples for class {k}:', len(v))\n",
    "\n",
    "    # display several examples of images from the training sample\n",
    "    if s == 'train':\n",
    "      random.seed(42)\n",
    "\n",
    "      imgs_path = random.sample(v, 4)\n",
    "      imgs_path = [os.path.join(data_path, f'{s}_signs/{k}_{p}') for p in imgs_path]\n",
    "\n",
    "      # read the image using the opencv library\n",
    "      imgs = [cv2.imread(p) for p in imgs_path]\n",
    "      # matplotlib expects img in RGB format but OpenCV provides it in BGR\n",
    "      # transform the BGR image into RGB\n",
    "      imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in imgs]\n",
    "\n",
    "      # create a figure for display\n",
    "      fig = plt.figure(figsize=(7, 2))\n",
    "      grid = ImageGrid(\n",
    "        fig, 111,\n",
    "        nrows_ncols=(1, 4)\n",
    "      )\n",
    "      # display the image\n",
    "      for ax, img in zip(grid, imgs):\n",
    "        ax.imshow(img)\n",
    "\n",
    "      fig.suptitle(f'Class {k}, {s.capitalize()} split')\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFqvlGUgozEH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIHmTVAL3O6d",
    "outputId": "92193703-0570-4e4c-ea96-74503f0388f5"
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_VNoBi13O6d"
   },
   "outputs": [],
   "source": [
    "input = torch.randn(20, 16, 50, 100) # batch_size, in_channels, h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epdGQ-zO3O6d",
    "outputId": "8b3ea710-06b2-49ad-c8e6-0cc5764ed4f5"
   },
   "outputs": [],
   "source": [
    "# pool of square window of kernel_size=3, stride=2\n",
    "m = nn.MaxPool2d(3, stride=2)\n",
    "output = m(input)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VPX_kz13O6d",
    "outputId": "653906d2-8f06-4266-d78a-5d7fbe235672"
   },
   "outputs": [],
   "source": [
    "# pool of non-square window\n",
    "m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
    "output = m(input)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCxHztlQH56H"
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKByJta73O6e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "orig_img = Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1LoEiQR3O6e"
   },
   "outputs": [],
   "source": [
    "def plot_examples(transformed_imgs:list, col_titles:list, cmap=None):\n",
    "\n",
    "    n_cols = len(transformed_imgs) +1\n",
    "    fig_size_x = 3 + len(transformed_imgs) * 1.5\n",
    "    fig, axs = plt.subplots(1, n_cols, figsize=(fig_size_x,2))\n",
    "\n",
    "    axs[0].imshow(orig_img)\n",
    "    axs[0].set_title('original image')\n",
    "\n",
    "    for i in range(len(transformed_imgs)):\n",
    "        axs[i+1].imshow(transformed_imgs[i], cmap=cmap)\n",
    "        axs[i+1].set_title(col_titles[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQxc_4GRIKg3"
   },
   "source": [
    "### resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBWX-fGK3O6f",
    "outputId": "aa7d992a-d402-43e6-a71b-535fdb6895cd"
   },
   "outputs": [],
   "source": [
    "resized_imgs = [T.Resize(size=size)(orig_img) for size in [32,128]]\n",
    "plot_examples(resized_imgs, ['32x32', '128x128'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylBxb82sJJRm"
   },
   "source": [
    "### Gray Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TluENrIi3O6f",
    "outputId": "d3501d88-e446-42c9-b796-879f0093f614"
   },
   "outputs": [],
   "source": [
    "gray_img = T.Grayscale()(orig_img)\n",
    "plot_examples([gray_img], [\"Gray\"], 'gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LOkbE_nJ5te"
   },
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jssnm7_S3O6g",
    "outputId": "78ed779c-095e-4dba-e121-6a77ac169915"
   },
   "outputs": [],
   "source": [
    "normalized_img = T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(T.ToTensor()(orig_img))\n",
    "normalized_img = [T.ToPILImage()(normalized_img)]\n",
    "plot_examples(normalized_img, [\"Standard normalize\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8QfFjqjKtoM"
   },
   "source": [
    "### Random Rotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdtPaGOi3O6h",
    "outputId": "7452fa7e-9c41-4014-dc0b-0758f352f89d"
   },
   "outputs": [],
   "source": [
    "rotated_imgs = [T.RandomRotation(degrees=d)(orig_img) for d in range(50,151,50)]\n",
    "plot_examples(rotated_imgs, [\"Rotation 50\",\"Rotation 100\",\"Rotation 150\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nav_zHmK_PD"
   },
   "source": [
    "### Center Crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJFYD3N_3O6h",
    "outputId": "0110e3e3-0e92-4a5c-87bc-867671d38b58"
   },
   "outputs": [],
   "source": [
    "center_crops = [T.CenterCrop(size=size)(orig_img) for size in (1280,640, 320)]\n",
    "plot_examples(center_crops,['1280x1280','640x640','320x320'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imA_5GxYLQcB"
   },
   "source": [
    "### Random Crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFylhI2e3O6h",
    "outputId": "f802ea8c-eff2-44ae-d710-7fe0db386800"
   },
   "outputs": [],
   "source": [
    "random_crops = [T.RandomCrop(size=size)(orig_img) for size in (832,704, 256)]\n",
    "plot_examples(random_crops,['832x832','704x704','256x256'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "392JxlevLcxR"
   },
   "source": [
    "### Gaussian Blur\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m6ucs9s3O6i",
    "outputId": "d0ede990-54a5-4adf-c532-ee17b624f06a"
   },
   "outputs": [],
   "source": [
    "downsized_img = T.Resize(size=512)(orig_img)\n",
    "blurred_imgs = [T.GaussianBlur(kernel_size=(51, 91), sigma=sigma)(downsized_img) for sigma in (3,7)]\n",
    "plot_examples(blurred_imgs, ['sigma=3', 'sigma=7'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06_EmDb6L8rI"
   },
   "source": [
    "### Gaussian Noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUuL-aVk3O6i",
    "outputId": "d60136d5-e6c7-4090-e0ca-4162127b9576"
   },
   "outputs": [],
   "source": [
    "def add_noise(inputs,noise_factor=0.3):\n",
    "    noisy = inputs+torch.randn_like(inputs) * noise_factor\n",
    "    noisy = torch.clip(noisy,0.,1.)\n",
    "    return noisy\n",
    "\n",
    "noise_imgs = [add_noise(T.ToTensor()(orig_img),noise_factor) for noise_factor in (0.3,0.6,0.9)]\n",
    "noise_imgs = [T.ToPILImage()(noise_img) for noise_img in noise_imgs]\n",
    "plot_examples(noise_imgs, [\"noise_factor=0.3\",\"noise_factor=0.6\",\"noise_factor=0.9\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj1IiVXhMkBm"
   },
   "source": [
    "### Random Blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNjgGBA93O6i",
    "outputId": "692a4979-2f3c-46cc-97aa-f9f8bdfc87b7"
   },
   "outputs": [],
   "source": [
    "def add_random_boxes(img,n_k,size=32):\n",
    "    h,w = size,size\n",
    "    img = np.asarray(img).copy()\n",
    "    img_size = img.shape[1]\n",
    "    boxes = []\n",
    "    for k in range(n_k):\n",
    "        y,x = np.random.randint(0,img_size-w,(2,))\n",
    "        img[y:y+h,x:x+w] = 0\n",
    "        boxes.append((x,y,h,w))\n",
    "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "    return img\n",
    "\n",
    "blocks_imgs = [add_random_boxes(orig_img,n_k=i, size=128) for i in (10,20)]\n",
    "plot_examples(blocks_imgs, [\"10 black boxes\",\"20 black boxes\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A4DEx_CSb-5"
   },
   "source": [
    "### -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeieTvwCRLeH"
   },
   "source": [
    "# Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B70Qx0iW3O6k",
    "outputId": "ec5b3ce9-74ab-4df1-a780-9deafc1b56d9"
   },
   "outputs": [],
   "source": [
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm1d(5) # num_features\n",
    "input = torch.randn(4, 5)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm1d(5, affine=False) # learnable parameters γ and β\n",
    "\n",
    "output = m(input)\n",
    "\n",
    "print('Input:\\n', input, '\\n\\n', f'mean {input.mean()}, std {input.std()}', '\\n')\n",
    "print('Output:\\n', input, '\\n\\n', f'mean {output.mean()}, std {output.std()}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XChM5m3C3O6l"
   },
   "outputs": [],
   "source": [
    "class SIGNSDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        #store filenames\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith('.jpg')]\n",
    "        #the first character of the filename contains the label\n",
    "        self.labels = [int(filename.split('/')[-1][0]) for filename in self.filenames]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        #return size of dataset\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #open image, apply transforms and return with label\n",
    "        image = Image.open(self.filenames[idx])  # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_fYEHy3O6l"
   },
   "outputs": [],
   "source": [
    "train_transformer = T.Compose([\n",
    "    T.Resize(64),              # resize the image to 64x64\n",
    "    T.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "    T.ToTensor()])             # transform it into a PyTorch Tensor\n",
    "\n",
    "test_transformer = T.Compose([\n",
    "    T.Resize(64),\n",
    "    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXZRhomU3O6l"
   },
   "outputs": [],
   "source": [
    "train_dataset = SIGNSDataset(f'{data_path}/train_signs/', train_transformer)\n",
    "test_dataset = SIGNSDataset(f'{data_path}/test_signs/', test_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpfe6lp_3O6m"
   },
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.num_channels = 32\n",
    "\n",
    "        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,\n",
    "        # stride, padding). We also include batch normalisation layers that help stabilise training.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n",
    "\n",
    "        # 2 fully connected layers to transform the output of the convolution layers to the final output\n",
    "        self.fc1 = nn.Linear(8*8*self.num_channels*4, self.num_channels*4)\n",
    "        self.fc2 = nn.Linear(self.num_channels*4, 6)\n",
    "\n",
    "    def forward(self, s):\n",
    "        #                                                  -> batch_size x 3 x 64 x 64\n",
    "        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3\n",
    "        s = self.conv1(s)                                   # batch_size x num_channels x 64 x 64\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 32 x 32\n",
    "        s = self.conv2(s)                                   # batch_size x num_channels*2 x 32 x 32\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 16 x 16\n",
    "        s = self.conv3(s)                                   # batch_size x num_channels*4 x 16 x 16\n",
    "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 8 x 8\n",
    "\n",
    "        # flatten the output for each image\n",
    "        s = s.view(-1, 8*8*self.num_channels*4)             # batch_size x 8*8*num_channels*4\n",
    "\n",
    "        # apply 2 fully connected layers with dropout\n",
    "        s = F.relu(self.fc1(s))                             # batch_size x self.num_channels*4\n",
    "        s = self.fc2(s)                                     # batch_size x 6\n",
    "\n",
    "        # apply log softmax on each image's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1-m_re33O6m"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(42)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Specify a computing device\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMfRUsKK3O6m"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4z9IEhx3O6m"
   },
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = BaselineModel().to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "146df9bd525c48eab6a55ee82250ed6a",
      "7efa44e5b35442ad914fc17bfc7e2000",
      "17ff33d1627c4b1e98a098631e87e00b",
      "ce184006c5d74e72a90aa0f463c14f0f",
      "a33d0b61869f49aa989f2ce06f9cc6a3",
      "a2556d1bfd4a4f81ab84cf60dc5318a5",
      "85f7680354694003b413cb2b01810292",
      "8ea7ec836c3e45a1886e0cf552261fe6",
      "64f40b05a9c04492897bda8855ccdd64",
      "9c505a73150f4b7794e4307d9f15ced7",
      "3bfd4441c2e0409391c798d642885d20",
      "2f5d6bcc4ed240fe92bf4ba3e0122735",
      "4049c292051c417eb02a2e739d8b9daf",
      "78d85faa69d94c4abe432456742c550c",
      "44c32fe9d5f3402080732b1534b6ecd8",
      "89eea899273e43ddb0d9535e65b2978a"
     ]
    },
    "id": "K6Jx_-__3O6n",
    "outputId": "dd14fe26-4456-4c32-ae21-942265a130c6"
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    # Train step\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    train_epoch_acc = []\n",
    "\n",
    "    for i, (train_batch, labels_batch) in tqdm(enumerate(train_dataloader)):\n",
    "        if cuda:\n",
    "            train_batch, labels_batch = train_batch.cuda(non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "\n",
    "        loss = criterion(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.cpu().detach().numpy()\n",
    "        batch_acc = balanced_accuracy_score(np.argmax(output_batch.cpu().detach().numpy(), axis=1),\n",
    "                                            labels_batch.cpu().detach().numpy())\n",
    "\n",
    "        train_epoch_loss.append(batch_loss)\n",
    "        train_epoch_acc.append(batch_acc)\n",
    "\n",
    "\n",
    "    print(f'Train epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(train_epoch_loss):.4f}, Acc: {np.mean(train_epoch_acc)}')\n",
    "    train_accs.append(np.mean(train_epoch_acc))\n",
    "    train_losses.append(np.mean(train_epoch_loss))\n",
    "\n",
    "    # Eval step\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_epoch_loss = []\n",
    "    test_epoch_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (test_batch, labels_batch) in enumerate(test_dataloader):\n",
    "            if cuda:\n",
    "                test_batch, labels_batch = test_batch.cuda(non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(test_batch)\n",
    "\n",
    "            loss = criterion(output_batch, labels_batch)\n",
    "\n",
    "            batch_loss = loss.cpu().detach().numpy()\n",
    "            batch_acc = balanced_accuracy_score(np.argmax(output_batch.cpu().detach().numpy(), axis=1),\n",
    "                                                labels_batch.cpu().detach().numpy())\n",
    "\n",
    "            test_epoch_loss.append(batch_loss)\n",
    "            test_epoch_acc.append(batch_acc)\n",
    "    print(f'Test epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(test_epoch_loss):.4f}, Acc: {np.mean(test_epoch_acc)}')\n",
    "    test_accs.append(np.mean(test_epoch_acc))\n",
    "    test_losses.append(np.mean(test_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExOcGamh3O6n",
    "outputId": "72623040-94a1-4fc1-c9ef-4d2a3ee8400b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNNLLLRK3O6n",
    "outputId": "5cc7ee70-12a8-48f6-8dda-3aa0b52c8caa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(train_accs, label='Train')\n",
    "plt.plot(test_accs, label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOnPg4C0caKU"
   },
   "source": [
    "### ***Моделювання з конспекту***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c3CupqRckdQ"
   },
   "source": [
    "***Створимо датасет, який буде репрезентувати наші дані.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu5N4rWr3O6o"
   },
   "outputs": [],
   "source": [
    "class SIGNSDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        # save file names\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith('.jpg')]\n",
    "        # the first character of the file name contains the class label\n",
    "        self.labels = [int(filename.split('/')[-1][0]) for filename in self.filenames]\n",
    "        # save the transformations (augmentations) that we will apply to the images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the size of the dataset\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # open the image, apply transformations and\n",
    "        # return an image with a class label\n",
    "        image = Image.open(self.filenames[idx])  # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMDOhRjhc4-m"
   },
   "source": [
    "***Визначимо трансформації, які ми застосуємо до зображень.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_bkcMhj3O6p"
   },
   "outputs": [],
   "source": [
    "train_transformer = T.Compose([\n",
    "    T.Resize(64),              # resizing the image to 64x64\n",
    "    T.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "    T.ToTensor()])             # transform it into a PyTorch Tensor\n",
    "\n",
    "eval_transformer = T.Compose([\n",
    "    T.Resize(64),\n",
    "    T.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZy5Ea7EdGpf"
   },
   "source": [
    "***Створюємо об’єкти Dataset:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8MI3Oc63O6p"
   },
   "outputs": [],
   "source": [
    "train_dataset = SIGNSDataset(f'{data_path}/train_signs/', train_transformer)\n",
    "test_dataset = SIGNSDataset(f'{data_path}/test_signs/', eval_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAgp1u86djS4"
   },
   "source": [
    "# ***Клас моделі***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI0zj67a3O6q"
   },
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "\n",
    "  def __init__(self, ):\n",
    "    super().__init__()\n",
    "    self.num_channels = 32\n",
    "\n",
    "    # convolution base\n",
    "    self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n",
    "    self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n",
    "\n",
    "    # classification layers\n",
    "    self.fc1 = nn.Linear(8*8*self.num_channels*4, self.num_channels*4)\n",
    "    self.fc2 = nn.Linear(self.num_channels*4, 6)\n",
    "\n",
    "  def forward(self, s):\n",
    "    #                         -> batch_size x 3 x 64 x 64\n",
    "    s = self.conv1(s)                  # batch_size x num_channels x 64 x 64\n",
    "    s = F.relu(F.max_pool2d(s, 2))           # batch_size x num_channels x 32 x 32\n",
    "    s = self.conv2(s)                  # batch_size x num_channels*2 x 32 x 32\n",
    "    s = F.relu(F.max_pool2d(s, 2))           # batch_size x num_channels*2 x 16 x 16\n",
    "    s = self.conv3(s)                  # batch_size x num_channels*4 x 16 x 16\n",
    "    s = F.relu(F.max_pool2d(s, 2))           # batch_size x num_channels*4 x 8 x 8\n",
    "\n",
    "    # flatten the output for each image\n",
    "    s = s.view(-1, 8*8*self.num_channels*4)       # batch_size x 8*8*num_channels*4\n",
    "\n",
    "    # apply 2 fully connected layers with dropout\n",
    "    s = F.relu(self.fc1(s))               # batch_size x self.num_channels*4\n",
    "    s = self.fc2(s)                   # batch_size x 6\n",
    "\n",
    "    return F.log_softmax(s, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvqHcH55eerd"
   },
   "source": [
    "### ***Тренування моделі***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93z46N0E3O6q"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J6SFhbV3O6q"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB4WBeQ13O6q"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsQ5jk7c3O6r"
   },
   "outputs": [],
   "source": [
    "model = BaselineModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jNSWwbG3O6r",
    "outputId": "6978b75c-3937-4e2c-b3f5-b9a14ddd477f"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boVHBm6W3O6r"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDA502OPgTyp"
   },
   "source": [
    "***Тренування моделі. Продовження***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIpj21V-3O6s"
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9pWz9No3O6t"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "num_epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4e093e6561c449118335f40f9a03dad1",
      "54fd46e927814e4ca6b1b3ed50c36ff5"
     ]
    },
    "id": "10ZCLkCC3O6t",
    "outputId": "b51aa17f-31ef-4b91-b448-e245f7b24061"
   },
   "outputs": [],
   "source": [
    "# Дослідити додатково!!!!!\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "  # Train step\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  train_epoch_loss = []\n",
    "  train_epoch_acc = []\n",
    "\n",
    "  for i, (train_batch, labels_batch) in tqdm(enumerate(train_dataloader)):\n",
    "    if cuda:\n",
    "\t\t    # non_blocking=True enables asynchronous data transfer to reduce runtime\n",
    "\t\t    train_batch, labels_batch = train_batch.cuda(non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "\n",
    "    # compute model output and loss\n",
    "    output_batch = model(train_batch)\n",
    "\n",
    "    loss = criterion(output_batch, labels_batch)\n",
    "\n",
    "    # clear previous gradients, compute gradients of all variables wrt loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # performs updates using calculated gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_loss = loss.cpu().detach().numpy()\n",
    "    batch_acc = balanced_accuracy_score(np.argmax(output_batch.cpu().detach().numpy(), axis=1),\n",
    "                      labels_batch.cpu().detach().numpy())\n",
    "\n",
    "    train_epoch_loss.append(batch_loss)\n",
    "    train_epoch_acc.append(batch_acc)\n",
    "\n",
    "\n",
    "  print(f'Train epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(train_epoch_loss):.4f}, Acc: {np.mean(train_epoch_acc)}')\n",
    "  train_accs.append(np.mean(train_epoch_acc))\n",
    "  train_losses.append(np.mean(train_epoch_loss))\n",
    "\n",
    "  # Eval step\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  test_epoch_loss = []\n",
    "  test_epoch_acc = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for i, (test_batch, labels_batch) in enumerate(test_dataloader):\n",
    "      if cuda:\n",
    "        test_batch, labels_batch = test_batch.cuda(non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "\n",
    "      # compute model output and loss\n",
    "      output_batch = model(test_batch)\n",
    "\n",
    "      loss = criterion(output_batch, labels_batch)\n",
    "\n",
    "      batch_loss = loss.cpu().detach().numpy()\n",
    "      batch_acc = balanced_accuracy_score(np.argmax(output_batch.cpu().detach().numpy(), axis=1),\n",
    "                        labels_batch.cpu().detach().numpy())\n",
    "\n",
    "      test_epoch_loss.append(batch_loss)\n",
    "      test_epoch_acc.append(batch_acc)\n",
    "  print(f'Test epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(test_epoch_loss):.4f}, Acc: {np.mean(test_epoch_acc)}')\n",
    "  test_accs.append(np.mean(test_epoch_acc))\n",
    "  test_losses.append(np.mean(test_epoch_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mndAcWsXhVY5"
   },
   "source": [
    "***Проаналізуємо графіки функції втрат та метрики точності.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fpcMk8N3O6u",
    "outputId": "3db05bb9-4d51-4951-edbe-b6fd0a0f551c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(train_accs, label='Train')\n",
    "plt.plot(test_accs, label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s43piH_3O6u",
    "outputId": "d7e94ff5-da9c-4558-c04a-42ba6020f7ae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Validation')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
