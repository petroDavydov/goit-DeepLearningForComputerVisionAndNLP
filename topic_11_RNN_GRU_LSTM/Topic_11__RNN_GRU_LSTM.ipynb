{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/arturshopengauer/topic-11-rnn-gru-lstm-keggle?scriptVersionId=288021175\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Завантаження необхідних бібліотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визначимо шлях до даних і пристрій, на якому будемо проводити розрахунки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/conll003-englishversion/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "будемо зчитувати тільки елементи на позиції 0 — слова — і 3 — мітки іменованих сутностей. У коді це буде відображатися так: sentences.append((l[0], l[3].strip('\\\\n')))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(filepath):\n",
    "    final = []\n",
    "    sentences = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if (line == ('-DOCSTART- -X- -X- O\\\\n') or line == '\\n'):\n",
    "                if len(sentences) > 0:\n",
    "                    final.append(sentences)\n",
    "                    sentences = []\n",
    "            else:\n",
    "                l = line.split(' ')\n",
    "                sentences.append((l[0], l[3].strip('\\n')))\n",
    "    return final\n",
    "    \n",
    "train_sents = load_sentences(data_path + 'train.txt')\n",
    "test_sents = load_sentences(data_path + 'test.txt')\n",
    "val_sents = load_sentences(data_path + 'valid.txt')\n",
    "\n",
    "train_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визначимо список міток класів і закодуємо їх для чисельного представлення.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "id2label = {str(i): label for i, label in enumerate(ner_labels)}\n",
    "label2id = {value: int(key) for key, value in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представимо наші завантажені речення як словник, де під ключем text будуть зберігатися наші речення, а під ключем label — відповідні мітки іменованих сутностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(samples):\n",
    "    df,label = [], []\n",
    "    for lines in samples:\n",
    "        cur_line, cur_label = list(zip(*lines))\n",
    "        df.append(list(cur_line))\n",
    "        label.append([label2id[i] for i in cur_label])\n",
    "    return {'text':df, 'label':label}\n",
    "    \n",
    "    \n",
    "train_df = get_df(train_sents)\n",
    "test_df = get_df(test_sents)\n",
    "val_df = get_df(val_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подальшої роботи з даними нам потрібно представити їх у чисельній формі.\n",
    "\n",
    "Спочатку побудуємо словник. Для цього спершу підрахуємо кількість появи кожного слова в корпусі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = defaultdict(int)\n",
    "\n",
    "for line in train_df['text']:\n",
    "    for word in line:\n",
    "        word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми не будемо використовувати слова, які дуже рідко з’являються, для тренування. Таким чином, ми зменшимо кількість неінформативних ознак у наборі даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_freq_word = []\n",
    "for k,v in word_dict.items():\n",
    "    if v < 2:\n",
    "        lower_freq_word.append(k)\n",
    "\n",
    "for word in lower_freq_word:\n",
    "    del word_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додамо до словника два спеціальні токени.\n",
    "Перший токен <UNK> позначатиме всі слова, які не присутні у словнику, так звані Out Of Vocabulary words, OOV words.\n",
    "Другий токен <PAD> позначає падинг (padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict['<UNK>'] = -1\n",
    "word_dict['<PAD>'] = -2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "створюємо словник, який буде містити слово та його індекс. Ми будемо використовувати цей словник, щоб представити наші речення в числовому вигляді для подальшої обробки нейронною мережею."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "\n",
    "for idx, word in enumerate(word_dict.keys()):\n",
    "  word2id[word] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset і DataLoader****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in to_ix.keys():\n",
    "            idxs.append(to_ix[w])\n",
    "        else:\n",
    "            idxs.append(to_ix['<UNK>'])\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишемо клас Dataset, необхідний для абстракції та організації даних під час навчання моделі. Він дозволяє легко керувати даними, завантажувати їх і забезпечує доступ до окремих прикладів даних і їхніх міток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['text']\n",
    "        self.labels = df['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_sequence(self.texts[item], word2id)\n",
    "        label = self.labels[item]\n",
    "        return {\n",
    "            'input_ids': inputs,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренування поточної моделі нам необхідно визначити collate-функцію"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, train):\n",
    "        self.train = train\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        if self.train:\n",
    "            output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [word2id['<PAD>']] for s in output[\"input_ids\"]]\n",
    "        if self.train:\n",
    "            output['labels'] = [s + (batch_max - len(s)) * [-100] for s in output[\"labels\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        if self.train:\n",
    "            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "    \n",
    "collate_fn = Collate(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Клас моделі**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. embeddings  шар ембедингів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. lstm — шар, який відповідає за Bi-LSTM-компонент у нашій мережі.\n",
    "\n",
    "\n",
    "\n",
    " - embedding_dim визначає розмірність вхідних векторів.\n",
    " - hidden_dim визначає розмірність прихованих станів і вихідного тензора.\n",
    " - bidirectional робить LSTM двонаправленою.\n",
    " - num_layers дозволяє створити глибоку модель з трьома послідовними LSTM-шарами.\n",
    " - batch_first вказує, що перший розмір вхідного тензора відповідає розміру батчу, що полегшує обробку даних у батчах.\n",
    " - fc створює повнозв'язний (лінійний) шар.\n",
    " - output_size визначає кількість нейронів у вихідному шарі, що відповідає кількості класів у задачі класифікації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        if embeddings is None:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=3, batch_first=True)\n",
    "\n",
    "        # 3. Dense Layer\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, batch_text):\n",
    "\n",
    "        embeddings = self.embeddings(batch_text)\n",
    "        \n",
    "        lstm_output, _ = self.lstm(embeddings) \n",
    "\n",
    "        logits = self.fc(lstm_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допоміжні функції для тренування"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оскільки нас цікавить тільки якість передбачення для міток іменованих сутностей, будемо прибирати з результатів токени зі значенням, меншим за 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_predictions_for_masked_items(predicted_labels, correct_labels): \n",
    "\n",
    "    predicted_labels_without_mask = []\n",
    "    correct_labels_without_mask = []\n",
    "        \n",
    "    for p, c in zip(predicted_labels, correct_labels):\n",
    "        if c > 0:\n",
    "            predicted_labels_without_mask.append(p)\n",
    "            correct_labels_without_mask.append(c)\n",
    "            \n",
    "    return predicted_labels_without_mask, correct_labels_without_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер визначимо функцію, відповідальну за навчання й валідацію. Як валідаційну метрику використаємо macro F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, batch_size, max_epochs, num_batches, patience, output_path):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)  # we mask the <pad> labels\n",
    "    optimizer = Adam(model.parameters())\n",
    "\n",
    "    train_f_score_history = []\n",
    "    dev_f_score_history = []\n",
    "    no_improvement = 0\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, total=num_batches, desc=f\"Epoch {epoch}\"):\n",
    "            \n",
    "            cur_batch_size, text_length = batch['input_ids'].shape\n",
    "            \n",
    "            pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "            gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n",
    "            \n",
    "            loss = criterion(pred, gold)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            \n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        train_score = f1_score(correct, predictions, average=\"macro\")\n",
    "        train_f_score_history.append(train_score)\n",
    "            \n",
    "        print(\"Total training loss:\", total_loss)\n",
    "        print(\"Training Macro F1:\", train_score)\n",
    "        \n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "\n",
    "                cur_batch_size, text_length = batch['input_ids'].shape\n",
    "                \n",
    "                pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "                gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n",
    "                \n",
    "                loss = criterion(pred, gold)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, pred_indices = torch.max(pred, 1)\n",
    "                predicted_labels = list(pred_indices.cpu().numpy())\n",
    "                correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n",
    "\n",
    "                predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                       correct_labels)\n",
    "\n",
    "                predictions += predicted_labels\n",
    "                correct += correct_labels\n",
    "\n",
    "        dev_score = f1_score(correct, predictions, average=\"macro\")\n",
    "            \n",
    "        print(\"Total validation loss:\", total_loss)\n",
    "        print(\"Validation Macro F1:\", dev_score)\n",
    "        \n",
    "        dev_f = dev_score\n",
    "        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n",
    "            no_improvement += 1\n",
    "\n",
    "        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n",
    "            print(\"Saving model.\")\n",
    "            torch.save(model, output_path)\n",
    "            no_improvement = 0\n",
    "            \n",
    "        if no_improvement > patience:\n",
    "            print(\"Validation F-score does not improve anymore. Stop training.\")\n",
    "            dev_f_score_history.append(dev_f)\n",
    "            break\n",
    "            \n",
    "        dev_f_score_history.append(dev_f)\n",
    "        \n",
    "    return train_f_score_history, dev_f_score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝ Зверніть увагу, тут ми додаємо механізм ранньої зупинки тренування."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визначимо функцію для тестування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter, batch_size, labels, target_names): \n",
    "    total_loss = 0\n",
    "    predictions, correct = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "    \n",
    "        for batch in test_iter:\n",
    "\n",
    "            cur_batch_size, text_length = batch['input_ids'].shape\n",
    "\n",
    "            pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "            gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n",
    "\n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "\n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "    \n",
    "    print(classification_report(correct, predictions, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Тренування моделі***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу визначимо гіперпараметри моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = len(id2label)\n",
    "MAX_EPOCHS = 50\n",
    "PATIENCE = 3\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = len(word2id)\n",
    "OUTPUT_PATH = \"/tmp/bilstmtagger\"\n",
    "num_batches = math.ceil(len(train_df) / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Створимо об’єкти Dataset і DataLoader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoNLLDataset(train_df)\n",
    "val_dataset = CoNLLDataset(val_df)\n",
    "test_dataset = CoNLLDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn,\n",
    "                              num_workers=4,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn,\n",
    "                              num_workers=4,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn,\n",
    "                              num_workers=4,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створимо об’єкт моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES) \n",
    "tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Нарешті, переходимо до тренування!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f, dev_f = train(tagger.to(device), train_loader, val_loader, BATCH_SIZE, MAX_EPOCHS, \n",
    "                       num_batches, PATIENCE, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Візуалізуємо навчальну й валідаційну метрики.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'epochs': range(0,len(train_f)), \n",
    "                  'train_f1': train_f, \n",
    "                   'dev_f1': dev_f})\n",
    "\n",
    "plt.plot('epochs', 'train_f1', data=df, color='blue', linewidth=2)\n",
    "plt.plot('epochs', 'dev_f1', data=df, color='green', linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо якість моделі на тестовому наборі даних. Завантажимо кращу зі збережених моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = torch.load(OUTPUT_PATH, weights_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо перевірку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(label2id.keys())[1:]\n",
    "label_idxs = list(label2id.values())[1:]\n",
    "\n",
    "test(tagger, test_loader, BATCH_SIZE, labels = label_idxs, target_names = labels)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 50445,
     "sourceId": 94327,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
